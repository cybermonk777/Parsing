{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30e5a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "180b9516",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u20bd' in position 16: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 122>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    125\u001b[0m sj_parse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\"\u001b[39m, parsed_data)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffers.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 128\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py:180\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m--> 180\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1251.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u20bd' in position 16: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "def salary_parser(salary: str) -> list:\n",
    "    \"\"\"\n",
    "    function parse salary string to min, max values and currency\n",
    "    \"\"\"\n",
    "    salary = \"\".join(salary.split())\n",
    "    money = re.findall(r\"\\d+\", salary)\n",
    "    currency = re.findall(r\"\\D+\", salary)[-1]\n",
    "    min_salary = money[0]\n",
    "    max_salary = money[-1] if money[-1] != min_salary else \"и больше\"\n",
    "    return [min_salary, max_salary, currency]\n",
    "\n",
    "\n",
    "def hh_parse(target: str, data: list) -> list:\n",
    "    \"\"\"\n",
    "    parse hh.ru vacancies searched by target\n",
    "    and save it to data list\n",
    "    \"\"\"\n",
    "    url = \"https://kazan.hh.ru/search/vacancy\"\n",
    "    params = {\n",
    "        \"from\": \"suggest_post\",\n",
    "        \"area\": \"2\",\n",
    "        \"text\": target,\n",
    "        \"ored_clusters\": \"true\",\n",
    "        \"enable_snippets\": \"true\",\n",
    "        \"hhtmFrom\": \"vacancy_search_list\",\n",
    "    }\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:108.0) Gecko/20100101 Firefox/108.0\"\n",
    "    }\n",
    "    page = 0\n",
    "    while True:\n",
    "        params[\"page\"] = page\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "        soup = bs(response.text, features=\"lxml\")\n",
    "        vacancies = soup.find_all(class_=\"vacancy-serp-item-body__main-info\")\n",
    "\n",
    "        if not vacancies:\n",
    "            break\n",
    "\n",
    "        for vacancy in vacancies:\n",
    "            salary_content = vacancy.find(\n",
    "                attrs={\"data-qa\": \"vacancy-serp__vacancy-compensation\"}\n",
    "            )\n",
    "            salary = salary_parser(salary_content.getText()) if salary_content else None\n",
    "            vacancy = vacancy.find(\"a\")\n",
    "            name = vacancy.getText()\n",
    "            link = vacancy.get(\"href\")\n",
    "\n",
    "            parsed_vacancy = {\"name\": name, \"link\": link, \"source\": \"hh.ru\"}\n",
    "            if salary:\n",
    "                parsed_vacancy[\"salary\"] = salary\n",
    "\n",
    "            parsed_data.append(parsed_vacancy)\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(5)\n",
    "    return data\n",
    "\n",
    "\n",
    "def sj_parse(target: str, data: list) -> list:\n",
    "    \"\"\"\n",
    "    parse superjob.ru vacancies searched by target\n",
    "    and save it to data list\n",
    "    \"\"\"\n",
    "    url = \"https://www.superjob.ru/vacancy/search/\"\n",
    "    base_url = \"https://kazan.superjob.ru\"\n",
    "    params = {\n",
    "        \"geo[t][0]\": \"14\",\n",
    "        \"geo[t][1]\": \"4\",\n",
    "        \"keywords\": target,\n",
    "    }\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:108.0) Gecko/20100101 Firefox/108.0\"\n",
    "    }\n",
    "    page = 0\n",
    "    while True:\n",
    "        params[\"page\"] = page\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        soup = bs(response.text, features=\"lxml\")\n",
    "        vacancies = soup.find_all(class_=\"f-test-search-result-item\")\n",
    "\n",
    "        if not vacancies:\n",
    "            break\n",
    "\n",
    "        for vacancy in vacancies:\n",
    "            salary_content = vacancy.find(class_=\"f-test-text-company-item-salary\")\n",
    "            vacancy = vacancy.find(\"a\")\n",
    "\n",
    "            # form filter\n",
    "            if not vacancy:\n",
    "                continue\n",
    "\n",
    "            link = vacancy.get(\"href\")\n",
    "\n",
    "            # advertising filter\n",
    "            if link.startswith(\"http\"):\n",
    "                continue\n",
    "\n",
    "            name = vacancy.getText()\n",
    "            parsed_vacancy = {\n",
    "                \"name\": name,\n",
    "                \"link\": base_url + link,\n",
    "                \"source\": \"kazan.superjob.ru\",\n",
    "            }\n",
    "\n",
    "            if salary_content:\n",
    "                salary_item = salary_content.span.getText()\n",
    "                salary = (\n",
    "                    salary_parser(salary_item)\n",
    "                    if re.findall(r\"\\d+\", salary_item)\n",
    "                    else None\n",
    "                )\n",
    "                if salary:\n",
    "                    parsed_vacancy[\"salary\"] = salary\n",
    "            parsed_data.append(parsed_vacancy)\n",
    "        page += 1\n",
    "        time.sleep(5)\n",
    "    return data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parsed_data = []\n",
    "    hh_parse(\"Аналитик\", parsed_data)\n",
    "    sj_parse(\"Python\", parsed_data)\n",
    "\n",
    "    with open(\"offers.json\", \"w\") as f:\n",
    "        json.dump(parsed_data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12926159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
